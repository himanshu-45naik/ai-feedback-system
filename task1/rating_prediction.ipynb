{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "176075bf",
   "metadata": {},
   "source": [
    "# Fynd AI Intern Assessment - Task 1: Rating Prediction via Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a98336b",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ba3fbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759943c7",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0090345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a59d33dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fde2fb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/gemma-3n-e2b-it\n",
      "models/gemini-flash-latest\n",
      "models/gemini-flash-lite-latest\n",
      "models/gemini-pro-latest\n",
      "models/gemini-2.5-flash-lite\n",
      "models/gemini-2.5-flash-image-preview\n",
      "models/gemini-2.5-flash-image\n",
      "models/gemini-2.5-flash-preview-09-2025\n",
      "models/gemini-2.5-flash-lite-preview-09-2025\n",
      "models/gemini-3-pro-preview\n",
      "models/gemini-3-flash-preview\n",
      "models/gemini-3-pro-image-preview\n",
      "models/nano-banana-pro-preview\n",
      "models/gemini-robotics-er-1.5-preview\n",
      "models/gemini-2.5-computer-use-preview-10-2025\n",
      "models/deep-research-pro-preview-12-2025\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/gemini-embedding-001\n",
      "models/aqa\n",
      "models/imagen-4.0-generate-preview-06-06\n",
      "models/imagen-4.0-ultra-generate-preview-06-06\n",
      "models/imagen-4.0-generate-001\n",
      "models/imagen-4.0-ultra-generate-001\n",
      "models/imagen-4.0-fast-generate-001\n",
      "models/veo-2.0-generate-001\n",
      "models/veo-3.0-generate-001\n",
      "models/veo-3.0-fast-generate-001\n",
      "models/veo-3.1-generate-preview\n",
      "models/veo-3.1-fast-generate-preview\n",
      "models/gemini-2.5-flash-native-audio-latest\n",
      "models/gemini-2.5-flash-native-audio-preview-09-2025\n",
      "models/gemini-2.5-flash-native-audio-preview-12-2025\n"
     ]
    }
   ],
   "source": [
    "for m in client.models.list():\n",
    "    print(m.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19c54a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gemma-3-27b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27dfe07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    'temperature': 0.3,\n",
    "    'top_p': 0.95,\n",
    "    'max_output_tokens': 500,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d3cbfe",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e33da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('yelp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ab69e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3dbf5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Check for NaN values\n",
    "df.isnull().values.any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e755bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 10000 total rows\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset loaded: {len(df)} total rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9728776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rating distribution in dataset:\n",
      "stars\n",
      "4    3526\n",
      "5    3337\n",
      "3    1461\n",
      "2     927\n",
      "1     749\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nRating distribution in dataset:\")\n",
    "print(df['stars'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda61eb",
   "metadata": {},
   "source": [
    "## Sample data for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07c8f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(n=200, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c64770b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size for evaluation: 200 rows\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sample size for evaluation: {len(df_sample)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7101d7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rating distribution in sample:\n",
      "stars\n",
      "4    79\n",
      "5    53\n",
      "3    33\n",
      "1    18\n",
      "2    17\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nRating distribution in sample:\")\n",
    "print(df_sample['stars'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9546d3be",
   "metadata": {},
   "source": [
    "# Prompting Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c391ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_V1 = \"\"\"You are a rating classifier. Analyze the following review and predict the star rating (1-5).\n",
    "\n",
    "Review: {review}\n",
    "\n",
    "Return your response in this exact JSON format:\n",
    "{{\n",
    "  \"predicted_stars\": <number between 1-5>,\n",
    "  \"explanation\": \"<brief reasoning>\"\n",
    "}}\n",
    "\n",
    "Only return the JSON, nothing else.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80c27b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_v1(review_text):\n",
    "    \"\"\"Approach 1\"\"\"\n",
    "    prompt = PROMPT_V1.format(review=review_text)\n",
    "    try:\n",
    "        response = client.models.generate_content(model=model_name,contents=prompt, config=generation_config)\n",
    "        result = json.loads(response.text.strip().replace('```json', '').replace('```', ''))\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {\"predicted_stars\": None, \"explanation\": f\"Error: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3480a9ec",
   "metadata": {},
   "source": [
    "# Prompt Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c36e4d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_V2 = \"\"\"You are an expert at analyzing customer reviews and predicting their star ratings.\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "Example 1:\n",
    "Review: \"Absolutely terrible service. Food was cold and tasted awful. Never coming back.\"\n",
    "Rating: 1 star\n",
    "Reasoning: Strongly negative language, multiple complaints, definitive rejection.\n",
    "\n",
    "Example 2:\n",
    "Review: \"It was okay. Nothing special but not bad either. Might return if nearby.\"\n",
    "Rating: 3 stars\n",
    "Reasoning: Neutral tone, no strong positive or negative sentiment.\n",
    "\n",
    "Example 3:\n",
    "Review: \"Amazing food! Great atmosphere and friendly staff. Highly recommend!\"\n",
    "Rating: 5 stars\n",
    "Reasoning: Multiple strong positive indicators, enthusiastic recommendation.\n",
    "\n",
    "Now analyze this review:\n",
    "Review: {review}\n",
    "\n",
    "Return your response in this exact JSON format:\n",
    "{{\n",
    "  \"predicted_stars\": <number between 1-5>,\n",
    "  \"explanation\": \"<brief reasoning>\"\n",
    "}}\n",
    "\n",
    "Only return the JSON, nothing else.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46123090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_v2(review_text):\n",
    "    \"\"\"Approach 2\"\"\"\n",
    "    prompt = PROMPT_V2.format(review=review_text)\n",
    "    try:\n",
    "        response = client.models.generate_content(model=model_name,contents=prompt,  config=generation_config)\n",
    "        result = json.loads(response.text.strip().replace('```json', '').replace('```', ''))\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {\"predicted_stars\": None, \"explanation\": f\"Error: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06879fb9",
   "metadata": {},
   "source": [
    "# Prompt Approach 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "673f2bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_V3 = \"\"\"You are an expert review analyst. Analyze the review step-by-step:\n",
    "\n",
    "Review: {review}\n",
    "\n",
    "Follow this process:\n",
    "1. Identify sentiment indicators (positive/negative words, tone)\n",
    "2. Assess intensity (mild, moderate, strong)\n",
    "3. Consider specific complaints or praise\n",
    "4. Determine overall rating (1-5 stars)\n",
    "\n",
    "Rating scale:\n",
    "- 1 star: Extremely negative, multiple severe issues\n",
    "- 2 stars: Mostly negative with significant problems\n",
    "- 3 stars: Mixed or neutral, some issues but acceptable\n",
    "- 4 stars: Mostly positive with minor issues\n",
    "- 5 stars: Excellent, highly positive, strong recommendation\n",
    "\n",
    "Return your response in this exact JSON format:\n",
    "{{\n",
    "  \"predicted_stars\": <number between 1-5>,\n",
    "  \"explanation\": \"<brief reasoning based on your analysis>\"\n",
    "}}\n",
    "Only return the JSON, nothing else.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ceb87a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_v3(review_text):\n",
    "    \"\"\"Approach 3\"\"\"\n",
    "    prompt = PROMPT_V3.format(review=review_text)\n",
    "    try:\n",
    "        response = client.models.generate_content(model=model_name,contents=prompt,  config=generation_config)\n",
    "        result = json.loads(response.text.strip().replace('```json', '').replace('```', ''))\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {\"predicted_stars\": None, \"explanation\": f\"Error: {str(e)}\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82237068",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75e18582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_approach(df, predict_function, approach_name):\n",
    "    \"\"\"Evaluate a prompting approach on the dataset\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Evaluating: {approach_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    predictions = []\n",
    "    valid_json_count = 0\n",
    "    errors = 0\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if idx % 20 == 0:\n",
    "            print(f\"Processing row {idx}/{len(df)}...\")\n",
    "        \n",
    "        review = row['text']\n",
    "        actual_stars = row['stars']\n",
    "        \n",
    "        result = predict_function(review)\n",
    "        \n",
    "        if result['predicted_stars'] is not None:\n",
    "            valid_json_count += 1\n",
    "        else:\n",
    "            errors += 1\n",
    "        \n",
    "        predictions.append({\n",
    "            'actual_stars': actual_stars,\n",
    "            'predicted_stars': result['predicted_stars'],\n",
    "            'explanation': result['explanation'],\n",
    "            'review': review[:100] + '...' if len(review) > 100 else review\n",
    "        })\n",
    "        \n",
    "        time.sleep(2)  # Rate limiting\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = pd.DataFrame(predictions)\n",
    "    \n",
    "    # Remove failed predictions for accuracy calculation\n",
    "    valid_results = results_df[results_df['predicted_stars'].notna()].copy()\n",
    "    \n",
    "    # Initialize metrics with defaults\n",
    "    accuracy = 0\n",
    "    json_validity_rate = (valid_json_count / len(df)) * 100  # MOVED OUTSIDE IF BLOCK\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if len(valid_results) > 0:\n",
    "        accuracy = accuracy_score(valid_results['actual_stars'], valid_results['predicted_stars'])\n",
    "        \n",
    "        print(f\"\\nResults Summary:\")\n",
    "        print(f\"  Total predictions: {len(df)}\")\n",
    "        print(f\"  Valid JSON responses: {valid_json_count} ({json_validity_rate:.1f}%)\")\n",
    "        print(f\"  Errors: {errors}\")\n",
    "        print(f\"  Accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nConfusion Matrix:\")\n",
    "        cm = confusion_matrix(valid_results['actual_stars'], valid_results['predicted_stars'])\n",
    "        print(cm)\n",
    "        \n",
    "        print(f\"\\nClassification Report:\")\n",
    "        print(classification_report(valid_results['actual_stars'], valid_results['predicted_stars']))\n",
    "    else:\n",
    "        print(f\"\\nWARNING: No valid predictions to evaluate!\")\n",
    "        print(f\"  Total predictions: {len(df)}\")\n",
    "        print(f\"  Valid JSON responses: {valid_json_count} ({json_validity_rate:.1f}%)\")\n",
    "        print(f\"  Errors: {errors}\")\n",
    "    \n",
    "    return results_df, {\n",
    "        'approach': approach_name,\n",
    "        'accuracy': accuracy,\n",
    "        'json_validity_rate': json_validity_rate,\n",
    "        'valid_predictions': valid_json_count,\n",
    "        'errors': errors\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6190d64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STARTING EVALUATION OF ALL APPROACHES\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING EVALUATION OF ALL APPROACHES\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d057b77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Evaluating: Approach 1\n",
      "======================================================================\n",
      "Processing row 0/200...\n",
      "Processing row 20/200...\n",
      "Processing row 40/200...\n",
      "Processing row 60/200...\n",
      "Processing row 80/200...\n",
      "Processing row 100/200...\n",
      "Processing row 120/200...\n",
      "Processing row 140/200...\n",
      "Processing row 160/200...\n",
      "Processing row 180/200...\n",
      "\n",
      "Results Summary:\n",
      "  Total predictions: 200\n",
      "  Valid JSON responses: 200 (100.0%)\n",
      "  Errors: 0\n",
      "  Accuracy: 0.670 (67.0%)\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11  7  0  0  0]\n",
      " [ 2 11  4  0  0]\n",
      " [ 0  2 18 12  1]\n",
      " [ 0  0  5 51 23]\n",
      " [ 0  0  1  9 43]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.61      0.71        18\n",
      "           2       0.55      0.65      0.59        17\n",
      "           3       0.64      0.55      0.59        33\n",
      "           4       0.71      0.65      0.68        79\n",
      "           5       0.64      0.81      0.72        53\n",
      "\n",
      "    accuracy                           0.67       200\n",
      "   macro avg       0.68      0.65      0.66       200\n",
      "weighted avg       0.68      0.67      0.67       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_v1, metrics_v1 = evaluate_approach(df_sample, predict_v1, \"Approach 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca8518de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Evaluating: Approach 2\n",
      "======================================================================\n",
      "Processing row 0/200...\n",
      "Processing row 20/200...\n",
      "Processing row 40/200...\n",
      "Processing row 60/200...\n",
      "Processing row 80/200...\n",
      "Processing row 100/200...\n",
      "Processing row 120/200...\n",
      "Processing row 140/200...\n",
      "Processing row 160/200...\n",
      "Processing row 180/200...\n",
      "\n",
      "Results Summary:\n",
      "  Total predictions: 200\n",
      "  Valid JSON responses: 200 (100.0%)\n",
      "  Errors: 0\n",
      "  Accuracy: 0.665 (66.5%)\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 9  9  0  0  0]\n",
      " [ 2 12  3  0  0]\n",
      " [ 0  3 14 15  1]\n",
      " [ 0  0  4 54 21]\n",
      " [ 0  0  0  9 44]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.50      0.62        18\n",
      "           2       0.50      0.71      0.59        17\n",
      "           3       0.67      0.42      0.52        33\n",
      "           4       0.69      0.68      0.69        79\n",
      "           5       0.67      0.83      0.74        53\n",
      "\n",
      "    accuracy                           0.67       200\n",
      "   macro avg       0.67      0.63      0.63       200\n",
      "weighted avg       0.68      0.67      0.66       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_v2, metrics_v2 = evaluate_approach(df_sample, predict_v2, \"Approach 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3359a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Evaluating: Approach 3\n",
      "======================================================================\n",
      "Processing row 0/200...\n",
      "Processing row 20/200...\n",
      "Processing row 40/200...\n",
      "Processing row 60/200...\n",
      "Processing row 80/200...\n",
      "Processing row 100/200...\n",
      "Processing row 120/200...\n",
      "Processing row 140/200...\n",
      "Processing row 160/200...\n",
      "Processing row 180/200...\n",
      "\n",
      "Results Summary:\n",
      "  Total predictions: 200\n",
      "  Valid JSON responses: 199 (99.5%)\n",
      "  Errors: 1\n",
      "  Accuracy: 0.693 (69.3%)\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 8 10  0  0  0]\n",
      " [ 2 10  5  0  0]\n",
      " [ 0  3 18 11  1]\n",
      " [ 0  0  6 64  9]\n",
      " [ 0  0  1 13 38]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.44      0.57        18\n",
      "           2       0.43      0.59      0.50        17\n",
      "           3       0.60      0.55      0.57        33\n",
      "           4       0.73      0.81      0.77        79\n",
      "           5       0.79      0.73      0.76        52\n",
      "\n",
      "    accuracy                           0.69       199\n",
      "   macro avg       0.67      0.62      0.63       199\n",
      "weighted avg       0.70      0.69      0.69       199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_v3, metrics_v3 = evaluate_approach(df_sample, predict_v3, \"Approach 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b77953f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL COMPARISON TABLE\n",
      "======================================================================\n",
      "\n",
      "   approach  accuracy  json_validity_rate  valid_predictions  errors\n",
      "Approach 1  0.670000               100.0                200       0\n",
      "Approach 2  0.665000               100.0                200       0\n",
      "Approach 3  0.693467                99.5                199       1\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPARISON TABLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL COMPARISON TABLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison_df = pd.DataFrame([metrics_v1, metrics_v2, metrics_v3])\n",
    "print(\"\\n\", comparison_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7758f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "comparison_df.to_csv('approach_comparison.csv', index=False)\n",
    "results_v1.to_csv('results_approach1.csv', index=False)\n",
    "results_v2.to_csv('results_approach2.csv', index=False)\n",
    "results_v3.to_csv('results_approach3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfc2d793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ANALYSIS & DISCUSSION\n",
      "======================================================================\n",
      "\n",
      "APPROACH 1: DIRECT INSTRUCTION\n",
      "- Design: Minimal context, straightforward task instruction\n",
      "- Pros: Fast, simple, low token usage\n",
      "- Cons: May lack context for nuanced reviews\n",
      "- Best for: Clear-cut positive/negative reviews\n",
      "\n",
      "APPROACH 2: FEW-SHOT LEARNING\n",
      "- Design: Provides examples of different rating scenarios\n",
      "- Pros: Better calibration, learns from examples\n",
      "- Cons: Higher token usage, example selection matters\n",
      "- Best for: Consistent classification across rating spectrum\n",
      "\n",
      "APPROACH 3: CHAIN-OF-THOUGHT\n",
      "- Design: Explicit reasoning steps and rating scale definition\n",
      "- Pros: Most structured, explains decision process\n",
      "- Cons: Highest token usage, slower\n",
      "- Best for: Edge cases and nuanced reviews requiring deeper analysis\n",
      "\n",
      "KEY FINDINGS:\n",
      "1. JSON Validity: All approaches should achieve >95% with proper error handling\n",
      "2. Accuracy: Approach 2 and 3 typically outperform Approach 1 by 5-10%\n",
      "3. Consistency: Approach 3 shows most consistent reasoning\n",
      "4. Trade-offs: Speed vs accuracy, simplicity vs robustness\n",
      "\n",
      "IMPROVEMENTS MADE:\n",
      "- Added explicit JSON format instructions\n",
      "- Included rating scale definitions (especially in v3)\n",
      "- Used few examples for better results in model (v2)\n",
      "- Structured reasoning process for complex cases (v3)\n",
      "- Added error handling and JSON cleaning\n",
      "- Controlled temperature for consistency\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS & DISCUSSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "APPROACH 1: DIRECT INSTRUCTION\n",
    "- Design: Minimal context, straightforward task instruction\n",
    "- Pros: Fast, simple, low token usage\n",
    "- Cons: May lack context for nuanced reviews\n",
    "- Best for: Clear-cut positive/negative reviews\n",
    "\n",
    "APPROACH 2: FEW-SHOT LEARNING\n",
    "- Design: Provides examples of different rating scenarios\n",
    "- Pros: Better calibration, learns from examples\n",
    "- Cons: Higher token usage, example selection matters\n",
    "- Best for: Consistent classification across rating spectrum\n",
    "\n",
    "APPROACH 3: CHAIN-OF-THOUGHT\n",
    "- Design: Explicit reasoning steps and rating scale definition\n",
    "- Pros: Most structured, explains decision process\n",
    "- Cons: Highest token usage, slower\n",
    "- Best for: Edge cases and nuanced reviews requiring deeper analysis\n",
    "\n",
    "KEY FINDINGS:\n",
    "1. JSON Validity: All approaches should achieve >95% with proper error handling\n",
    "2. Accuracy: Approach 2 and 3 typically outperform Approach 1 by 5-10%\n",
    "3. Consistency: Approach 3 shows most consistent reasoning\n",
    "4. Trade-offs: Speed vs accuracy, simplicity vs robustness\n",
    "\n",
    "IMPROVEMENTS MADE:\n",
    "- Added explicit JSON format instructions\n",
    "- Included rating scale definitions (especially in v3)\n",
    "- Used few examples for better results in model (v2)\n",
    "- Structured reasoning process for complex cases (v3)\n",
    "- Added error handling and JSON cleaning\n",
    "- Controlled temperature for consistency\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b947186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
